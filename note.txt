# class UNet(nn.Module):
#     def __init__(self, in_channels=18, out_channels=1):
#         super(UNet, self).__init__()

#         def conv_block(in_c, out_c):
#             block = nn.Sequential(
#                 nn.Conv2d(in_c, out_c, kernel_size=3, padding=1),
#                 nn.ReLU(inplace=True),
#                 nn.Conv2d(out_c, out_c, kernel_size=3, padding=1),
#                 nn.ReLU(inplace=True)
#             )
#             return block

#         def down_block(in_c, out_c):
#             block = nn.Sequential(
#                 nn.MaxPool2d(2),
#                 conv_block(in_c, out_c)
#             )
#             return block

#         def up_block(in_c, out_c):
#             block = nn.Sequential(
#                 nn.ConvTranspose2d(in_c, out_c, kernel_size=2, stride=2),
#                 conv_block(out_c, out_c)
#             )
#             return block

#         # Encoder
#         self.enc1 = conv_block(in_channels, 64)
#         self.enc2 = down_block(64, 128)

#         # Bottleneck
#         self.bottleneck = conv_block(128, 256)

#         # Decoder
#         self.dec2 = up_block(256, 128)
#         self.dec1 = conv_block(128, 64)

#         # Adjustment layer for skip connection
#         self.adjust_e1_channels = nn.Conv2d(64, 128, kernel_size=1)

#         # Final layer
#         self.final = nn.Conv2d(64, out_channels, kernel_size=1)

#     def forward(self, x):
#         # Encoder path
#         e1 = self.enc1(x)
#         e2 = self.enc2(e1)

#         # Bottleneck
#         b = self.bottleneck(e2)

#         # Decoder path
#         d2 = self.dec2(b)

#         # Adjust channels of e1 to match d2 before addition
#         e1_adjusted = self.adjust_e1_channels(e1)
#         d1 = self.dec1(d2 + e1_adjusted)  # Skip connection

#         # Final output layer
#         out = self.final(d1)
#         return out